

"""
Описание Python скрипта для генерации видео-скриптов с помощью LangChain, Ollama и Pydantic.

Этот скрипт демонстрирует использование библиотеки LangChain для взаимодействия
с большой языковой моделью (LLM), запущенной локально через Ollama.
Цель скрипта - автоматизировать создание контента для коротких видеороликов
на основе новостного текста о банкротстве или изменениях в законодательстве.

Основные компоненты и логика работы:

1.  **Импорты и Конфигурация:**
    * Импортируются необходимые классы из библиотек `langchain_core`, `langchain_community`, `pydantic`.
    * Задается имя модели Ollama (`OLLAMA_MODEL`), которая будет использоваться для генерации текста (например, 'llama3', 'mistral'). Предполагается, что сервер Ollama запущен локально.

2.  **Определение Структуры Вывода (`VideoScriptOutput`):**
    * С помощью `pydantic.BaseModel` создается класс `VideoScriptOutput`.
    * Этот класс строго определяет **желаемую структуру** конечного результата: он должен содержать два поля - `headline` (строка, заголовок) и `video_script` (строка, текст скрипта).
    * `pydantic.Field` используется для добавления описаний к полям, что помогает LLM лучше понять, какой именно текст ожидается в каждом поле.

3.  **Парсер Вывода (`PydanticOutputParser`):**
    * Создается экземпляр `PydanticOutputParser`, связанный с моделью `VideoScriptOutput`.
    * Задача парсера - взять "сырой" текстовый ответ от LLM и преобразовать его в структурированный объект Python (`VideoScriptOutput`).
    * Парсер также генерирует специальные инструкции (`format_instructions`), которые вставляются в промпт, чтобы указать LLM, в каком формате должен быть представлен ответ (обычно это JSON-подобная структура).

4.  **Шаблон Промпта (`ChatPromptTemplate`):**
    * Создается шаблон промпта, который будет отправлен LLM.
    * Используется `ChatPromptTemplate` для структурирования запроса (например, как сообщение от пользователя).
    * Промпт содержит:
        * Описание **роли** для LLM (копирайтер/маркетолог).
        * **Контекст** задачи (обработка новостей).
        * Четкие **инструкции** по генерации заголовка и скрипта (длина, содержание, стиль, CTA).
        * Место для вставки **входного текста новости** (`{news_text}`).
        * Место для вставки **инструкций по форматированию** от парсера (`{format_instructions}`).

5.  **Инициализация LLM (`ChatOllama`):**
    * Создается объект `ChatOllama`, который обеспечивает соединение с указанной моделью (`OLLAMA_MODEL`) на локально запущенном сервере Ollama.
    * `temperature` - параметр, контролирующий случайность/креативность ответа LLM.

6.  **Создание Цепочки (`chain`):**
    * Используется LangChain Expression Language (LCEL) с оператором `|` (пайп) для соединения компонентов в последовательную цепочку обработки:
        * `prompt`: Формирует окончательный текст запроса к LLM, подставляя входные данные и инструкции по форматированию.
        * `llm`: Отправляет сформированный промпт в Ollama и получает ответ.
        * `parser`: Обрабатывает ответ LLM и преобразует его в объект `VideoScriptOutput`.

7.  **Входные Данные (`news_input`):**
    * Пример текста новости, который будет обработан.

8.  **Выполнение Цепочки (`chain.invoke`):**
    * Цепочка запускается с передачей входных данных (текста новости).
    * `invoke` последовательно выполняет все шаги цепочки (промпт -> LLM -> парсер).

9.  **Вывод Результата:**
    * Результат работы цепочки (объект `VideoScriptOutput`) сохраняется в переменной `result`.
    * Скрипт выводит сгенерированный заголовок и текст видеоскрипта в структурированном виде.
    * Включен блок `try...except` для отлова возможных ошибок (например, если Ollama не запущен или модель недоступна).

Таким образом, скрипт автоматизирует процесс генерации креативного контента (заголовка и скрипта для видео) из структурированной информации (новостной текст), используя локальную LLM и обеспечивая получение результата в предсказуемом формате благодаря Pydantic и LangChain.
"""
